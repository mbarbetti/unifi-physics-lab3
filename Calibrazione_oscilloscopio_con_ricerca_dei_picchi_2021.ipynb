{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Calibrazione oscilloscopio con ricerca dei picchi 2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbarbetti/unifi-physics-lab3/blob/main/Calibrazione_oscilloscopio_con_ricerca_dei_picchi_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB7USURe5X5w"
      },
      "source": [
        "Calibrazione temporale dell'oscilloscopio con ricerca dei picchi \n",
        "===\n",
        "Lo scopo di questo programma è ricavare i coefficienti lineari per la conversione tra la misura del TDC implementato in LabView, che indicheremo come **unità aribitrarie** (*a.u.*), e i nanosecondi (*ns*). Il fit lineare ai picchi ottenuti dai segnali del TIME CALIBRATOR ci permetterà così di correggere le misure dei tempi da eventuali **errori sistematici** dovuti alla catena elettronica, all'oscilloscopio o a LabVIEW. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2mEYmWQ5WCN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksVwYzsb7Tmj"
      },
      "source": [
        "**Prima di tutto dovete modificare il codice inserendo il nome del gruppo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u0aYLKX_qMl"
      },
      "source": [
        "## INSERIRE NUMERO DEL GRUPPO\n",
        "group = \"N2\"   # esempio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGoBfbtJjBZD"
      },
      "source": [
        "Sfruttiamo il file di testo (\".txt\") per preparare un `DataFrame` Pandas, cioè una tabella organizzata per righe e colonne avente un set di funzioni utili per cercare, modificare, aggiungere o rimuovere elementi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sro8tJFX7dJF"
      },
      "source": [
        "file_path = f\"https://raw.githubusercontent.com/mbarbetti/unifi-physics-lab3/main/data/2021/data_{group}.txt\"\n",
        "data = pd.read_csv (file_path, header = 4, delim_whitespace = True).drop([0])\n",
        "data['QDC1'] = -data['QDC1'] * 1e9\n",
        "data['QDC2'] = -data['QDC2'] * 1e9\n",
        "data['TDC'] = data['TDC'] * 1e9\n",
        "print(data)   # dataframe\n",
        "print(min(data['QDC1']),\" \",min(data['QDC1']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INpwcoiNAjS6"
      },
      "source": [
        "Qui si riempiono degli istogrammi bi- e uni-dimensionali con i valori del QDC1 e QDC2 (rispettivamente muone ed elettrone). Le unità sono arbitrarie e sono moltiplicate per 1e09 per comodità di visualizzazione. Queste distribuzioni servono per selezionare gli eventi di fisica rispetto a quelli di calibrazione: guardando allo scatter plot **dovete decidere dove mettere i tagli per selezionare gli eventi corrispondenti al TIME CALIBRATOR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwytWjRX1Uqj"
      },
      "source": [
        "# Plot calib values\n",
        "### SET THE QDC RANGES FOR TIME CALIBRATOR SIGNALS \n",
        "qdc1_max = 3.6\n",
        "qdc2_max = 3.6\n",
        "\n",
        "data_calib = data.query (f\"(QDC1 < {qdc1_max}) & (QDC2 < {qdc2_max})\")\n",
        "qdc1 = data_calib['QDC1']\n",
        "qdc2 = data_calib['QDC2']\n",
        "qdc1_all = data['QDC1']\n",
        "qdc2_all = data['QDC2']\n",
        "\n",
        "plt.xlabel (\"QDC1\")\n",
        "plt.ylabel (\"QDC2\")\n",
        "plt.plot ([0,10], [qdc2_max, qdc2_max], color = \"red\", lw = 2)\n",
        "plt.plot ([qdc1_max, qdc1_max], [0,10], color = \"red\", lw = 2)\n",
        "plt.scatter (qdc1_all, qdc2_all, s=1)\n",
        "plt.axis ([2,10,2,8])\n",
        "plt.show()\n",
        "\n",
        "nbin = 100\n",
        "\n",
        "plt.xlabel (\"QDC1\")\n",
        "plt.ylabel (\"Entries\")\n",
        "plt.hist (qdc1_all, bins=nbin, range = (2,10), alpha = 0.75, label='QDC1')\n",
        "plt.hist (qdc1, bins=nbin, range = (2,10), alpha = 0.75, label='QDC1')\n",
        "plt.yscale(\"log\")\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel (\"QDC2\")\n",
        "plt.ylabel (\"Entries\")\n",
        "plt.hist (qdc2_all, bins=nbin, range = (2,10), alpha = 0.75, label='QDC2')\n",
        "plt.hist (qdc2, bins=nbin, range = (2,10), alpha = 0.75, label='QDC2')\n",
        "plt.yscale(\"log\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WxB0hVmCQ7O"
      },
      "source": [
        "### Prepare and fill histogram of time intervals\n",
        "tdc = data_calib[\"TDC\"].values\n",
        "\n",
        "tmin = tdc.min()\n",
        "tmax = tdc.max()\n",
        "#nbins = tmax-tmin\n",
        "nbins = 500\n",
        "\n",
        "plt.figure (figsize = (10,8), dpi = 100)\n",
        "plt.xlabel('Time interval [ns]')\n",
        "plt.ylabel('Number of events')\n",
        "plt.hist (data_calib['TDC'], bins=nbins, range = (tmin, tmax), label='Measured time intervals')\n",
        "plt.yscale(\"linear\")\n",
        "plt.show()\n",
        "\n",
        "#prepare date for peak finder \n",
        "nbins = int(data_calib['TDC'].max() + 1)\n",
        "hist = np.histogram(tdc, bins=nbins, range=[0,nbins])\n",
        "print(hist[0])\n",
        "print(hist[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61C66by7yC3K"
      },
      "source": [
        "I tempi di riferimento per la calibrazione sono quelli prodotti dal **TIME CALIBRATOR**  che produce segnali di _start_ e _stop_ ogni 160 ns (o l'eventuale altro valore da voi impostato sul TIME CALIBRATOR) in una finestra temporale di circa 10.24 $\\mu$s. Disponendo i tempi misurati dall'oscilloscopio in un istogramma, quello che ci aspettiamo di osservare è un \"pettine\" con 64 denti corrispondente ai 64 valori dei tempi generati dal TIME CALIBRATOR:\n",
        "\n",
        "<center>$\\mathcal{N}_{peaks} = \\frac{\\rm{finestra \\, temporale}}{1/\\rm{frequenza}} = \\frac{10240 \\, \\rm{ns}}{160 \\, \\rm{ns}} = 64$</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuqRB5WaD951"
      },
      "source": [
        "freq = 160   # time in ns\n",
        "t_cal = np.arange (1,65) * freq\n",
        "t_cal   # TIME CALIBRATOR values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez0MfHYS1muc"
      },
      "source": [
        "La cella di codice sottostante permette di _fittare_ con una gaussiana un singolo picco contenuto nel \"pettine\".  \n",
        "La funzione `gauss_fit` richiede i seguenti attributi:\n",
        "* `sample`: un campione di misure dei tempi relativo a uno specifico picco;\n",
        "* `bins`: numero di bins per costruire l'istogramma su cui fare il fit gaussiano;\n",
        "* `range`: intervallo di valori scelto per l'istogramma su cui fare il fit gaussiano;\n",
        "* `plot`: variabile booleana per abilitare (`True`) o meno (`False`) il plot del fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7gpDbAwzto5"
      },
      "source": [
        "from scipy.optimize import curve_fit\n",
        "\n",
        "def gauss (x, mean, sigma, N):\n",
        "  return (N / np.sqrt (2 * np.pi * sigma**2)) * np.exp (-0.5 * (x - mean)**2 / sigma**2)\n",
        "\n",
        "def gauss_fit (sample, bins, range, plot = False):\n",
        "  weights = np.ones_like (sample) / len (sample)\n",
        "  entries, bin_edges = np.histogram (sample, bins = bins, range = range, weights = weights)\n",
        "  #tot_entries = np.sum (entries)\n",
        "  #print (tot_entries)\n",
        "\n",
        "  bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2.\n",
        "  min_bin = bin_centers.min()\n",
        "  max_bin = bin_centers.max()\n",
        "  width = max_bin - min_bin\n",
        "\n",
        "  popt, pcov = curve_fit (gauss, bin_centers, entries, bounds = ([min_bin, 0.1*width, 0.], [max_bin, width, 1.]))\n",
        "  perr = np.sqrt (np.diag (pcov))\n",
        "\n",
        "  if plot:\n",
        "    plt.figure (figsize = (8,5))\n",
        "    plt.title ('Fit plot', fontsize = 14)\n",
        "    plt.xlabel ('ADC counts [a.u.]', fontsize = 12)\n",
        "    plt.ylabel ('Normalized entries', fontsize = 12)\n",
        "    plt.hist (sample, bins = bins, range = range, weights = weights, color = 'royalblue', label = 'Data')\n",
        "    plt.plot (bin_centers, gauss (bin_centers, popt[0], popt[1], popt[2]), color = 'red', linewidth = 2, label = 'Fit result')\n",
        "    plt.legend (loc = 'upper left', fontsize = 10)\n",
        "    plt.show()\n",
        "\n",
        "  return popt, perr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3_3zpygHcPS"
      },
      "source": [
        "La funzione `peak_study` permette di estrarre **sotto-campioni** di tempi misurati a partire dai dati contenuti in `sample`. Per far ciò, preso la cella di codice sottostante estrae il sotto-campione tra i valori `lower_edge` e `upper_edge` . Il campione così ottenuto viene quindi utilizzato per **caratterizzare** il picco. Se la _flag_ booleana `fit` è vera, allora il sotto-campione viene passato alla funzione `gauss_fit` che ne calcola centroide e deviazione standard (con relativi errori dovuti al fit). Se invece la flag booleana `fit` è falsa, allora `peak_study` ha come output la media e la deviazione standard dei valori contenuti nel sotto-campione (senza errori)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIHjRVbv8JMl"
      },
      "source": [
        "def peak_study (sample, lower_edge, upper_edge, plot = False, fit = True):\n",
        "#  mask = (sample > (time_val - bound)) & (sample < (time_val + bound))\n",
        "  mask = (sample > lower_edge) & (sample < upper_edge)\n",
        "  sub_sample = sample [mask]\n",
        "  #print(sub_sample)\n",
        "\n",
        "  min_val = sub_sample.min()\n",
        "  max_val = sub_sample.max()\n",
        "  width = max_val - min_val\n",
        "  #print(min_val, max_val, width)\n",
        "\n",
        "  bins  = 50\n",
        "  _range = [min_val - 0.5 * width, max_val + 0.5 * width]\n",
        "\n",
        "  if plot and not fit:\n",
        "    print ('\\n----- DATA PLOT -----')\n",
        "    plt.figure (figsize = (8,5))\n",
        "    plt.title  ('Time distribution', fontsize = 14)\n",
        "    plt.xlabel ('Measured time', fontsize = 12)\n",
        "    plt.ylabel ('Entries', fontsize = 12)\n",
        "    plt.hist (sub_sample, bins = bins, range = _range, color = 'royalblue')\n",
        "    plt.yscale(\"log\")\n",
        "    plt.show()\n",
        "\n",
        "  if fit and plot:\n",
        "    if plot: print ('\\n----- FIT PLOT -----')\n",
        "    results, errs = gauss_fit (sub_sample, bins, _range, plot = plot)\n",
        "    mean, sigma, N = results\n",
        "    mean_err, sigma_err, N_err = errs\n",
        "  elif not fit:\n",
        "    mean = np.mean (sub_sample)\n",
        "    mean_err = 0.\n",
        "    sigma = np.std (sub_sample)\n",
        "    sigma_err = 0.\n",
        "\n",
        "  mean_fit  = [mean, mean_err]\n",
        "  sigma_fit = [sigma, sigma_err]\n",
        "  return mean_fit, sigma_fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncf_kPVyKQu4"
      },
      "source": [
        "Il codice che segue individua i picchi in un campione di dati. Attivando la _flag_ booleana `verbose` è possibile visualizzare i picchi individuati con sovrapposta la curva gaussiana ottenuta dal fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeLKexxNOoaO"
      },
      "source": [
        "### Algorithm for finding peaks in histogram of ADC counts\n",
        "### The histogram is scanned, and an integration window is chosen around each peak to perform the gaussian fit\n",
        "def findPeaks(hist, verbose=False):\n",
        "    peaks = []\n",
        "    RSEs = []\n",
        "    starting_edge = 1\n",
        "    edge = 2\n",
        "    done = False\n",
        "    headroom = 10\n",
        "\n",
        "    while not done:\n",
        "        integral = 0\n",
        "        previous_integral = 0\n",
        "\n",
        "        bin_content = hist[0][edge-1]\n",
        "        next_bin_content = hist[0][edge]\n",
        "        previous_integral = np.sum(hist[0][starting_edge:edge-1])\n",
        "        integral = np.sum(hist[0][starting_edge:edge])\n",
        "\n",
        "        if (integral > 0 and previous_integral < (edge-starting_edge) and starting_edge != edge -1) :\n",
        "            starting_edge = edge - 1\n",
        "            integral = np.sum(hist[0][starting_edge:edge])\n",
        "\n",
        "        if (bin_content > next_bin_content  and integral > 10):\n",
        "            for j in range(headroom):\n",
        "                integral_temp = integral\n",
        "                integral = np.sum(hist[0][starting_edge:edge+1+j])\n",
        "                if (integral - integral_temp < 1) :\n",
        "                    edge = edge + j \n",
        "                    break\n",
        "\n",
        "            if (verbose):\n",
        "              print(\"New peak found:\")\n",
        "              print(f\"from {starting_edge} to {edge}\")\n",
        "              print(f\"integral = {integral}\") \n",
        "\n",
        "            mean, sigma = peak_study(tdc, starting_edge, edge, plot=True if verbose else False)\n",
        "            weighted_avg = np.average(hist[1][starting_edge:edge], weights=hist[0][starting_edge:edge])\n",
        "            peaks.append(mean[0])\n",
        "            RSEs.append(mean[1])\n",
        "            starting_edge = edge\n",
        "        \n",
        "        edge = edge +1\n",
        "\n",
        "        if edge >= len(hist[0]) :\n",
        "            done = True\n",
        "        \n",
        "    \n",
        "    return peaks, RSEs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd5YV_9kLG_c"
      },
      "source": [
        "Chimando la funzione `findPeaks()` sui dati del TIME CALIBRATOR è possibile ottenere tutti i **centroidi** dei picchi che formano il \"pettine\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrH9p-3_9byL"
      },
      "source": [
        "t_mis = list()\n",
        "errs  = list()\n",
        "\n",
        "#times = np.linspace (tmin, tmax, len(t_cal))\n",
        "#print (times)\n",
        "\n",
        "t_mis, errs = findPeaks(hist, True)\n",
        "\n",
        "df = pd.DataFrame (np.c_ [t_cal, t_mis, errs], columns = ['Times', 'ADC counts', 'Uncertainties'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp9HIGaWLr_M"
      },
      "source": [
        "A questo punto abbiamo tutto il necessario per procedere con il **fit lineare**!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPGr7zK0A0dI"
      },
      "source": [
        "ones = np.ones_like (t_cal)\n",
        "\n",
        "X = np.c_ [t_cal, ones]\n",
        "V = np.diag (np.square (errs))\n",
        "V_inv = np.linalg.inv (V)\n",
        "\n",
        "B = np.linalg.inv (X.T @ V_inv @ X) @ X.T @ V_inv\n",
        "theta = B @ t_mis\n",
        "U = B @ V @ B.T\n",
        "sigmas = np.diag(U)\n",
        "sigmas = np.sqrt(sigmas)\n",
        "print ('Results from least square fit:')\n",
        "print (f'    Slope = {theta[0]:.6f} +/- {sigmas[0]:.6f} a.u./ns')\n",
        "print (f'    Intercept = {theta[1]:.5f} +/- {sigmas[1]:.5f} a.u.')\n",
        "print ('\\nCorrelation matrix:')\n",
        "print (U)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz8YboXiMNFt"
      },
      "source": [
        "Verifichiamo la bontà del fit *plottando* i risultati ottenuti!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw5uSzmdIa0b"
      },
      "source": [
        "plt.figure (figsize = (10, 7))\n",
        "plt.title ('Time calibration', fontsize = 14)\n",
        "plt.xlabel ('True time [ns]', fontsize = 12)\n",
        "plt.ylabel ('Measured time [ns]', fontsize = 12)\n",
        "\n",
        "t_fit = theta[0] * t_cal + theta[1]\n",
        "\n",
        "plt.errorbar (t_cal, t_mis, yerr = errs, color = 'blue', fmt = 'v',\n",
        "              markersize = 4, barsabove = True, capsize = 2, label = 'Data points')\n",
        "plt.plot (t_cal, t_fit, color = 'red', linewidth = 1, label = 'Calibration fit')\n",
        "\n",
        "plt.legend (loc = 'upper left', fontsize = 10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSPoy8xgQVXJ"
      },
      "source": [
        "L'ampia finestra considerata per i tempi non permette di apprezzare eventuali _disaccordi_ con quanto ottenuto dal fit. Per evidenziarli, riportiamo su grafico i cosiddetti **residui**:\n",
        "\n",
        "<center>$\\rm{residuals} = y_{\\rm{true}} - y_{\\rm{fit}}$</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZMXDV-hLYxM"
      },
      "source": [
        "plt.figure (figsize = (10, 7))\n",
        "plt.title ('Chi square residuals', fontsize = 14)\n",
        "plt.xlabel ('ADC counts [a.u.]', fontsize = 12)\n",
        "plt.ylabel ('Residuals', fontsize = 12)\n",
        "\n",
        "residuals = t_mis - t_fit\n",
        "\n",
        "plt.errorbar (t_mis, residuals, yerr = errs, color = 'blue', fmt = 'v',\n",
        "              markersize = 4, barsabove = True, capsize = 2, label = 'Residuals [a.u.]')\n",
        "#plt.plot (t_mis, 100 * residuals / np.array(t_mis), color = 'green', \n",
        "#          linewidth = 1, label = 'Residuals [%]')\n",
        "\n",
        "plt.hlines (0, t_mis[0], t_mis[-1], color = 'red', linestyle='dashed')\n",
        "\n",
        "plt.legend (loc = 'lower right', fontsize = 10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWx2oFhtRV8L"
      },
      "source": [
        "Quindi, sfruttando la libreria `stats` di SciPy, calcoliamo il $\\chi^2$ associato al fit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8AYZT6zNIyj"
      },
      "source": [
        "from scipy.stats import chi2\n",
        "\n",
        "squares  = np.square (residuals/errs)\n",
        "chi2_fit = np.sum (squares)\n",
        "ndf = squares.size - 2\n",
        "p_val = 1 - chi2.cdf (chi2_fit, ndf)\n",
        "\n",
        "print ('Results from least square fit:')\n",
        "print (f'    Slope = {theta[0]:.5f} a.u./ns')\n",
        "print (f'    Intercept = {theta[1]:.5f} a.u.')\n",
        "print (f'    chi2 = {chi2_fit:.3f}')\n",
        "print (f'    ndf  = {ndf:.3f}')\n",
        "print (f'    p-value = {100*p_val:.1f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8A4CAyzSVZK"
      },
      "source": [
        "Infine, siamo in grado di calcolare il **fattore di conversione** da unità arbitrarie a nanosecondi, così come siamo in grado di **correggere** le misure dei tempi per tener conto del contributo sistematico introdotto dalla catena di misura. **Qual è l'errore sui due parametri `a` e `b`?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3fhLik0eU_U"
      },
      "source": [
        "a = 1 / theta[0]\n",
        "b = - theta[1] * conv_factor   #errore? \n",
        "\n",
        "print ('Results for muon lifetime study:')\n",
        "print ('    a = {:.5f} ns/a.u.' . format (a))\n",
        "print ('    b = {:.5f} ns' . format (b))\n",
        "\n",
        "print ('\\nt_corr = ({:.5f}) * t_mis + ({:.5f}) ns' . format (a, b))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}